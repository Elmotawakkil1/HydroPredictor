{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10626579,"sourceType":"datasetVersion","datasetId":6579481}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install catboost ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:31:34.249129Z","iopub.execute_input":"2025-02-09T13:31:34.249371Z","iopub.status.idle":"2025-02-09T13:31:38.647862Z","shell.execute_reply.started":"2025-02-09T13:31:34.249338Z","shell.execute_reply":"2025-02-09T13:31:38.647017Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.7)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.5)\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\nRequirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\nRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.16.0->catboost) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.16.0->catboost) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.16.0->catboost) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.16.0->catboost) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.16.0->catboost) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.16.0->catboost) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0,>=1.16.0->catboost) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0,>=1.16.0->catboost) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0,>=1.16.0->catboost) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0,>=1.16.0->catboost) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0,>=1.16.0->catboost) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#!pip install optuna\n!pip install optuna catboost tensorflow imblearn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T13:31:46.941809Z","iopub.execute_input":"2025-02-09T13:31:46.942168Z","iopub.status.idle":"2025-02-09T13:31:50.651284Z","shell.execute_reply.started":"2025-02-09T13:31:46.942106Z","shell.execute_reply":"2025-02-09T13:31:50.650020Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\nRequirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.7)\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\nCollecting imblearn\n  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.5)\nRequirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\nRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.24.1)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.17.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\nRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from imblearn) (0.12.4)\nRequirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\nRequirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn->imblearn) (3.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.2.0)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optuna) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\nDownloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\nInstalling collected packages: imblearn\nSuccessfully installed imblearn-0.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport optuna\nfrom catboost import CatBoostClassifier\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, matthews_corrcoef, recall_score\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras import regularizers, initializers\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\n\n# Ensure reproducibility\ndef set_random_seeds(seed=1):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nset_random_seeds()\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Disable GPU\n\n# âœ… Hybrid Model: CatBoost + MLP + Meta-Classifier\nclass HybridHydroPredictor:\n    def __init__(self, learning_rate=0.01, epochs=25, catboost_depth=6, top_features=10, model_weight=0.5, **kwargs): # add model_weight here\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.catboost_depth = catboost_depth\n        self.top_features = top_features\n        self.model_weight = model_weight # add model_weight here\n        self.selected_features = None\n        self.mlp_model = None\n        self.catboost_model = None\n        self.meta_classifier = None\n\n    def fit(self, X_train, y_train):\n        \"\"\"Train CatBoost, select features, train MLP, and combine predictions using a meta-classifier.\"\"\"\n        smote = SMOTE(sampling_strategy=\"not minority\", random_state=1)\n        X_train, y_train = smote.fit_resample(X_train, y_train)\n\n        # Step 1: Train CatBoost\n        self.catboost_model = CatBoostClassifier(\n            depth=self.catboost_depth,\n            iterations=70,\n            l2_leaf_reg=9,\n            learning_rate=self.learning_rate,\n            random_state=1,\n            verbose=0,\n            task_type=\"CPU\"\n        )\n        self.catboost_model.fit(X_train, y_train)\n\n        # Step 2: Extract CatBoost Leaf Embeddings\n        catboost_embeddings = self.catboost_model.calc_leaf_indexes(X_train)\n\n        # Step 3: Train MLP on CatBoost Embeddings\n        self._fit_mlp(catboost_embeddings, y_train)\n\n        # Step 4: Train Meta-Classifier on the outputs of CatBoost and MLP\n        catboost_preds = self.catboost_model.predict_proba(X_train)\n        mlp_preds = self.mlp_model.predict(catboost_embeddings, verbose=0)\n        combined_features = np.hstack((catboost_preds, mlp_preds))\n\n        self.meta_classifier = LogisticRegression(random_state=1)\n        self.meta_classifier.fit(combined_features, y_train)\n\n    def _fit_mlp(self, X_train, y_train):\n        \"\"\"Train the MLP\"\"\"\n        num_classes = len(np.unique(y_train))\n        y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n\n        tf.random.set_seed(1)\n\n        self.mlp_model = Sequential([\n            tf.keras.Input(shape=(X_train.shape[1],)),\n            Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.01)),\n            BatchNormalization(),\n            Dropout(0.4),\n            Dense(32, activation='relu', kernel_initializer=initializers.he_normal(seed=1), kernel_regularizer=regularizers.l2(0.01)),\n            BatchNormalization(),\n            Dropout(0.3),\n            Dense(num_classes, activation='softmax', kernel_initializer=initializers.he_normal(seed=1))\n        ])\n\n        self.mlp_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n                               loss='categorical_crossentropy',\n                               metrics=['accuracy'])\n\n        self.mlp_model.fit(X_train, y_train, epochs=self.epochs, batch_size=16, verbose=0)\n\n    def predict(self, X):\n        \"\"\"Hybrid prediction using a meta-classifier\"\"\"\n        catboost_embeddings = self.catboost_model.calc_leaf_indexes(X)\n        catboost_preds = self.catboost_model.predict_proba(X)\n        mlp_preds = self.mlp_model.predict(catboost_embeddings, verbose=0)\n        combined_features = np.hstack((catboost_preds, mlp_preds))\n        return self.meta_classifier.predict(combined_features)\n\n# âœ… Data Preprocessing\ndef load_and_preprocess(filepath):\n    df = pd.read_csv(filepath, sep=\";\", decimal=',')\n    df = df.sort_values(by=df.columns.tolist())\n\n    le = LabelEncoder()\n    y = le.fit_transform(df[\"label\"].values)\n    X = df.drop(columns=[\"label\"]).values\n\n    random.seed(1)  # Lock the seed for data operations\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    X_train = np.round(X_train, 6)\n    X_test = np.round(X_test, 6)\n\n    return X_train, X_test, y_train, y_test\n\n# âœ… Optuna Optimization\ndef objective(trial, X_train, y_train, X_test, y_test):\n    params = {\n        'learning_rate': trial.suggest_float(\"learning_rate\", 0.02975968404658167, 0.02975968404658167, log=True),\n        'catboost_depth': trial.suggest_int(\"catboost_depth\", 10, 10),\n        'top_features': trial.suggest_int(\"top_features\", 10, 10),\n        'epochs': trial.suggest_int(\"epochs\", 20, 20),\n        'model_weight': trial.suggest_float(\"model_weight\", 0.13725204227553417, 0.13725204227553417),\n        'l2_leaf_reg': trial.suggest_int(\"l2_leaf_reg\", 10, 10),\n        'iterations': trial.suggest_int(\"iterations\", 163, 163),\n        'random_state': trial.suggest_int(\"random_state\", 1, 1)\n    }\n\n    random.seed(1)\n    set_random_seeds(1)\n\n    model = HybridHydroPredictor(**params)\n\n    skf = StratifiedKFold(n_splits=5, shuffle=False)\n    f1_scores = []\n    for train_idx, val_idx in skf.split(X_train, y_train):\n        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n\n        model.fit(X_fold_train, y_fold_train)\n        y_pred = model.predict(X_fold_val)\n        f1_scores.append(f1_score(y_fold_val, y_pred, average='weighted'))\n\n    avg_f1 = np.mean(f1_scores)\n\n    model.fit(X_train, y_train)\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    def calculate_metrics(y_true, y_pred):\n        acc = accuracy_score(y_true, y_pred)\n        f1 = f1_score(y_true, y_pred, average='weighted')\n        kappa = cohen_kappa_score(y_true, y_pred)\n        recall = recall_score(y_true, y_pred, average='weighted')\n        mcc = matthews_corrcoef(y_true, y_pred)\n        return acc, f1, kappa, recall, mcc\n\n    train_acc, train_f1, train_kappa, train_recall, train_mcc = calculate_metrics(y_train, y_train_pred) \n    test_acc, test_f1, test_kappa, test_recall, test_mcc = calculate_metrics(y_test, y_test_pred)\n    print(f\"Trial {trial.number}:, epoch {params['epochs']}, iterations {params['iterations']}, learning_rate {params['learning_rate']}, model_weight {params['model_weight']}, catboost_depth {params['catboost_depth']}, l2_leaf_reg {params['l2_leaf_reg']}\" )\n    print(f\"Training â†’ Acc: {train_acc:.4f}, F1: {train_f1:.4f}, Kappa: {train_kappa:.4f}, Recall: {train_recall:.4f}, MCC: {train_mcc:.4f}\")    \n    print(f\"Testing â†’ Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Kappa: {test_kappa:.4f}, Recall: {test_recall:.4f}, MCC: {test_mcc:.4f}\")\n\n    return avg_f1\n\n# âœ… Main Script\nif __name__ == \"__main__\":\n    filepath = \"/kaggle/input/gwpmapp/Reference 3 classes without xy.csv\"\n    X_train, X_test, y_train, y_test = load_and_preprocess(filepath)\n\n    study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner(),\n                                sampler=optuna.samplers.TPESampler(seed=1))\n    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=50)\n\n    best_params = study.best_trial.params\n    model = HybridHydroPredictor(**best_params)\n\n    smote = SMOTE(sampling_strategy=\"not minority\", random_state=1)\n    X_train, y_train = smote.fit_resample(X_train, y_train)\n\n    model.fit(X_train, y_train)\n\n    print(\"\\nðŸ”¹ **Final Model Performance**\")\n    y_test_pred = model.predict(X_test)\n    print(f\"ðŸ“Œ Testing â†’ Accuracy: {accuracy_score(y_test, y_test_pred):.4f}, F1: {f1_score(y_test, y_test_pred, average='weighted'):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T01:26:38.332892Z","iopub.execute_input":"2025-02-02T01:26:38.333205Z","iopub.status.idle":"2025-02-02T01:27:38.260112Z","shell.execute_reply.started":"2025-02-02T01:26:38.333181Z","shell.execute_reply":"2025-02-02T01:27:38.258746Z"}},"outputs":[{"name":"stderr","text":"[I 2025-02-02 01:26:38,378] A new study created in memory with name: no-name-b11e860a-75a0-4077-b6a0-5d8b96dbc3b0\n[I 2025-02-02 01:27:18,109] Trial 0 finished with value: 0.8452033414161878 and parameters: {'learning_rate': 0.02975968404658167, 'catboost_depth': 10, 'top_features': 10, 'epochs': 20, 'model_weight': 0.13725204227553417, 'l2_leaf_reg': 10, 'iterations': 163, 'random_state': 1}. Best is trial 0 with value: 0.8452033414161878.\n","output_type":"stream"},{"name":"stdout","text":"Trial 0:, epoch 20, iterations 163, learning_rate 0.02975968404658167, model_weight 0.13725204227553417, catboost_depth 10, l2_leaf_reg 10\nTraining â†’ Acc: 0.9073, F1: 0.9070, Kappa: 0.8610, Recall: 0.9073, MCC: 0.8631\nTesting â†’ Acc: 0.8615, F1: 0.8636, Kappa: 0.7923, Recall: 0.8615, MCC: 0.7932\n","output_type":"stream"},{"name":"stderr","text":"[W 2025-02-02 01:27:38,203] Trial 1 failed with parameters: {'learning_rate': 0.02975968404658167, 'catboost_depth': 10, 'top_features': 10, 'epochs': 20, 'model_weight': 0.13725204227553417, 'l2_leaf_reg': 10, 'iterations': 163, 'random_state': 1} because of the following error: KeyboardInterrupt().\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n  File \"<ipython-input-18-1db96564899d>\", line 184, in <lambda>\n    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=50)\n  File \"<ipython-input-18-1db96564899d>\", line 151, in objective\n    model.fit(X_fold_train, y_fold_train)\n  File \"<ipython-input-18-1db96564899d>\", line 63, in fit\n    self._fit_mlp(catboost_embeddings, y_train)\n  File \"<ipython-input-18-1db96564899d>\", line 95, in _fit_mlp\n    self.mlp_model.fit(X_train, y_train, epochs=self.epochs, batch_size=16, verbose=0)\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 318, in fit\n    for step, iterator in epoch_iterator.enumerate_epoch():\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 689, in enumerate_epoch\n    iterator = iter(self._distributed_dataset)\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 501, in __iter__\n    return iterator_ops.OwnedIterator(self)\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 709, in __init__\n    self._create_iterator(dataset)\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 748, in _create_iterator\n    gen_dataset_ops.make_iterator(ds_variant, self._iterator_resource)\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 3478, in make_iterator\n    _result = pywrap_tfe.TFE_Py_FastPathExecute(\nKeyboardInterrupt\n[W 2025-02-02 01:27:38,206] Trial 1 failed with value None.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-1db96564899d>\u001b[0m in \u001b[0;36m<cell line: 178>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m     study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner(),\n\u001b[1;32m    183\u001b[0m                                 sampler=optuna.samplers.TPESampler(seed=1))\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-1db96564899d>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    182\u001b[0m     study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner(),\n\u001b[1;32m    183\u001b[0m                                 sampler=optuna.samplers.TPESampler(seed=1))\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-1db96564899d>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0my_fold_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fold_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fold_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fold_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fold_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mf1_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_fold_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-1db96564899d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Step 3: Train MLP on CatBoost Embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcatboost_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Step 4: Train Meta-Classifier on the outputs of CatBoost and MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-1db96564899d>\u001b[0m in \u001b[0;36m_fit_mlp\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     93\u001b[0m                                metrics=['accuracy'])\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36menumerate_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 for step in range(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 709\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    746\u001b[0m             self._flat_output_types)\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3476\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3479\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3480\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport optuna\nfrom catboost import CatBoostClassifier\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score,  matthews_corrcoef, recall_score\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import initializers\nfrom imblearn.over_sampling import SMOTE  # Changed to SMOTE\nimport matplotlib.pyplot as plt\n\n# Ensure reproducibility\ndef set_random_seeds(seed=1):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    # Disable XLA: This line ensures XLA is always disabled\n    os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n    os.environ['MKL_NUM_THREADS'] = '1'\n    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n    \n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nset_random_seeds()\n\n# Disable GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n\n# Hybrid Model: MLP + CatBoost\nclass HybridHydroPredictor:\n    def __init__(self, learning_rate=0.01, epochs=25, catboost_depth=6, model_weight=0.5, **kwargs):\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.catboost_depth = catboost_depth\n        self.model_weight = model_weight\n        self.selected_features = None  # To store selected features\n        self.mlp_model = None\n        self.catboost_model = None\n        \n    def fit(self, X_train, y_train):\n        \"\"\"Train both MLP and CatBoost\"\"\"\n        # Apply SMOTE before feature selection\n        # Lock the random state within SMOTE:\n        smote = SMOTE(sampling_strategy=\"not minority\", random_state=1)  \n        X_train, y_train = smote.fit_resample(X_train, y_train)\n\n        # Feature selection - keep all features for now (or use alternative methods)\n        self.selected_features = np.arange(X_train.shape[1])  # Keep all features\n        \n        # Train MLP\n        self._fit_mlp(X_train, y_train)\n        \n        # Train CatBoost\n        self.catboost_model = CatBoostClassifier(\n            depth=self.catboost_depth,\n            iterations=75,\n            l2_leaf_reg=9,\n            learning_rate=self.learning_rate,\n            random_state=1,  # Lock the random state within CatBoost\n            verbose=0,\n            task_type=\"CPU\"\n        )\n        self.catboost_model.fit(X_train, y_train)\n\n    def _fit_mlp(self, X_train, y_train):\n        \"\"\"Train the MLP\"\"\"\n        # Operations within layers have been moved to functions to ensure reproducibility:\n        def dense_layer(units, activation, kernel_initializer, kernel_regularizer, input_shape=None):\n            layer = Dense(units, activation=activation, kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)\n            if input_shape is not None:\n                layer.build(input_shape=input_shape)  # Build the layer if input_shape is provided\n            return layer\n\n        def dropout_layer(rate, seed):\n            layer = Dropout(rate, seed=seed)\n            return layer\n            \n        num_classes = len(np.unique(y_train))\n        y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n\n        # Set Seed Before MLP Model Creation\n        tf.random.set_seed(1)  # Set the TensorFlow seed before creating the model\n\n        self.mlp_model = Sequential([\n            tf.keras.Input(shape=(X_train.shape[1],)),\n            Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.0005, l2=0.0005)),  # Added L1/L2 here\n            BatchNormalization(),\n            dropout_layer(0.4, seed=1),  \n            dense_layer(32, 'relu', initializers.he_normal(seed=1), regularizers.l2(0.0005)),\n            BatchNormalization(),\n            dropout_layer(0.3, seed=1),  \n            dense_layer(num_classes, 'softmax', initializers.he_normal(seed=1), None)  # No regularizer for the output layer\n        ])\n\n        self.mlp_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n                              loss='categorical_crossentropy',\n                              metrics=['accuracy'])\n\n        self.mlp_model.fit(X_train, y_train, epochs=self.epochs, batch_size=16, verbose=0)  # Always train for the given number of epochs \n\n    def predict(self, X):\n        \"\"\"Hybrid prediction\"\"\"\n        X = X[:, self.selected_features]  # Use selected features for prediction\n        mlp_probs = self.mlp_model.predict(X, verbose=0)\n        catboost_probs = self.catboost_model.predict_proba(X)\n        combined_probs = (self.model_weight * mlp_probs) + ((1 - self.model_weight) * catboost_probs)\n        return np.argmax(combined_probs, axis=1)\n\n\n# Data Preprocessing\ndef load_and_preprocess(filepath):\n    df = pd.read_csv(filepath, sep=\";\", decimal=',')\n    df = df.sort_values(by=df.columns.tolist())  # Sort for consistent order\n\n    le = LabelEncoder()\n    y = le.fit_transform(df[\"label\"].values)\n    X = df.drop(columns=[\"label\"]).values\n\n    # Global Seed: Set random seed at the beginning\n    random.seed(1)  # Lock the seed for data operations\n\n    # Split once, use the same split for everything\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n\n    # Apply StandardScaler\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    # Rounding to 6 decimal places\n    X_train = np.round(X_train,6)\n    X_test = np.round(X_test, 6)\n\n    return X_train, X_test, y_train, y_test\n\n\n# Optuna Optimization (Modified - Removed Caching)\ndef objective(trial, X_train, y_train, X_test, y_test):\n    params = {\n        'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 0.029703031164113368, log=True),  # Fixed interval\n        'model_weight': trial.suggest_float(\"model_weight\",  0.28892133270076946,  0.28892133270076946),  # Fixed interval\n        'catboost_depth': trial.suggest_int(\"catboost_depth\", 11,11),  # Fixed interval\n    }\n\n\n\n    # Local Seed Before Model Creation\n    random.seed(1)  # Ensures consistent model initialization (CatBoost)\n    set_random_seeds(1) # Ensures consistent model initialization (TensorFlow)\n\n    model = HybridHydroPredictor(**params)\n\n    # Perform cross-validation (same as before)\n    skf = StratifiedKFold(n_splits=5, shuffle=False)  # Removed shuffle\n    f1_scores = []\n    for train_idx, val_idx in skf.split(X_train, y_train):\n        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n\n        model.fit(X_fold_train, y_fold_train)\n        y_pred = model.predict(X_fold_val)\n        f1_scores.append(f1_score(y_fold_val, y_pred, average='weighted'))\n\n    avg_f1 = np.mean(f1_scores)\n\n    # Train on full training data and evaluate on test data\n    # Train on full training data and evaluate on test data\n    model.fit(X_train, y_train)  # Always train on full training data\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    # Calculate metrics for training and testing\n    def calculate_metrics(y_true, y_pred):\n        acc = accuracy_score(y_true, y_pred)\n        f1 = f1_score(y_true, y_pred, average='weighted')\n        kappa = cohen_kappa_score(y_true, y_pred)\n        recall = recall_score(y_true, y_pred, average='weighted')\n        mcc = matthews_corrcoef(y_true, y_pred)\n        return acc, f1, kappa, recall, mcc\n\n    train_acc, train_f1, train_kappa, train_recall, train_mcc = calculate_metrics(y_train, y_train_pred)\n    test_acc, test_f1, test_kappa, test_recall, test_mcc = calculate_metrics(y_test, y_test_pred)\n\n    # Print training and testing results for each trial\n    print(f\"Trial {trial.number}: \")\n    print(f\"Training â†’ Acc: {train_acc:.4f}, F1: {train_f1:.4f}, Kappa: {train_kappa:.4f}, Recall: {train_recall:.4f}, MCC: {train_mcc:.4f} \")    \n    print(f\"Avg â†’ Acc: {np.mean(train_acc):.4f}, F1: {np.mean(train_f1):.4f}, Kappa: {np.mean(train_kappa):.4f}, Recall: {np.mean(train_recall):.4f}, MCC: {np.mean(train_mcc):.4f}\")\n    print(f\"Testing â†’ Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Kappa: {test_kappa:.4f}, Recall: {test_recall:.4f}, MCC: {test_mcc:.4f}\")\n\n    return avg_f1  # Return the average F1 score across folds\n\n# Main Script\nif __name__ == \"__main__\":\n    filepath = \"/kaggle/input/gwpmapp/Reference 3 classes without xy.csv\"\n    X_train, X_test, y_train, y_test = load_and_preprocess(filepath)\n\n    study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner(),\n                                sampler=optuna.samplers.TPESampler(seed=1))\n    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=50)\n\n    best_params = study.best_trial.params\n    model = HybridHydroPredictor(**best_params)\n\n    # Apply SMOTE to the full training set\n    smote = SMOTE(sampling_strategy=\"not minority\", random_state=1)\n    X_train, y_train = smote.fit_resample(X_train, y_train)\n\n    model.fit(X_train, y_train)\n\n    # Evaluate the model\n    print(\"\\nðŸ”¹ **Final Model Performance**\")\n    y_test_pred = model.predict(X_test)\n    print(\n        f\"ðŸ“Œ Testing â†’ Accuracy: {accuracy_score(y_test, y_test_pred):.4f}, F1: {f1_score(y_test, y_test_pred, average='weighted'):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T18:30:30.782651Z","iopub.execute_input":"2025-02-01T18:30:30.782962Z"}},"outputs":[{"name":"stderr","text":"[I 2025-02-01 18:30:30,845] A new study created in memory with name: no-name-f60fc42c-d80b-433e-abd4-5ee339950fc8\n[I 2025-02-01 18:31:01,794] Trial 0 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 0: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:31:32,594] Trial 1 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 1: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:32:03,146] Trial 2 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 2: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:32:33,753] Trial 3 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 3: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:33:04,693] Trial 4 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 4: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:33:35,544] Trial 5 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 5: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:34:06,523] Trial 6 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 6: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:34:37,652] Trial 7 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 7: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:35:08,359] Trial 8 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 8: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:35:39,087] Trial 9 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 9: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:36:09,633] Trial 10 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 10: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:36:40,305] Trial 11 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 11: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:37:10,900] Trial 12 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 12: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:37:41,745] Trial 13 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 13: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:38:12,738] Trial 14 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 14: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:38:43,640] Trial 15 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 15: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:39:14,350] Trial 16 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 16: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:39:44,973] Trial 17 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 17: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:40:15,669] Trial 18 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 18: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:40:46,638] Trial 19 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 19: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:41:17,411] Trial 20 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 20: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:41:47,908] Trial 21 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 21: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:42:18,543] Trial 22 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 22: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:42:49,048] Trial 23 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 23: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:43:40,814] Trial 24 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 24: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:44:14,520] Trial 25 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 25: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:44:46,499] Trial 26 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 26: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:45:17,976] Trial 27 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 27: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:45:49,782] Trial 28 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 28: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:46:20,642] Trial 29 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 29: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:46:51,273] Trial 30 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 30: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:47:22,002] Trial 31 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 31: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-02-01 18:47:52,806] Trial 32 finished with value: 0.8142275499848164 and parameters: {'learning_rate': 0.029703031164113367, 'model_weight': 0.2889213327007695, 'catboost_depth': 11}. Best is trial 0 with value: 0.8142275499848164.\n","output_type":"stream"},{"name":"stdout","text":"Trial 32: \nTraining â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474 \nAvg â†’ Acc: 0.8958, F1: 0.8954, Kappa: 0.8436, Recall: 0.8958, MCC: 0.8474\nTesting â†’ Acc: 0.8308, F1: 0.8335, Kappa: 0.7462, Recall: 0.8308, MCC: 0.7470\n","output_type":"stream"}],"execution_count":null}]}